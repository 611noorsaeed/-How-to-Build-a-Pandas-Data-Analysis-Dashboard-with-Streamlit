{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/noorsaeed/paraphrase-generation-using-t5-paws-dataset?scriptVersionId=208345727\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Dataset Loading","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Load the PAWS dataset\ndataset = load_dataset(\"paws\", \"labeled_final\")\n\ndataset[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"# Convert to DataFrame and filter paraphrases (label = 1)\ndef preprocess_paws(dataset, label=1):\n    df = pd.DataFrame(dataset)\n    df = df[df['label'] == label]\n    df['input_text'] = \"paraphrase: \" + df['sentence1']\n    df['target_text'] = df['sentence2']\n    return df[['input_text', 'target_text']]\n\ntrain_data = preprocess_paws(dataset['train']).sample(3000, random_state=42)\ntest_data = preprocess_paws(dataset['test'])\nvalidation_data = preprocess_paws(dataset['validation'])\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_data)\ntest_dataset = Dataset.from_pandas(test_data)\nvalidation_dataset = Dataset.from_pandas(validation_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"# Initialize tokenizer and model\nmodel_name = \"t5-base\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Tokenization function\ndef tokenize_function(examples):\n    inputs = tokenizer(examples['input_text'], max_length=128, truncation=True, padding=\"max_length\")\n    targets = tokenizer(examples['target_text'], max_length=128, truncation=True, padding=\"max_length\")\n    inputs['labels'] = targets['input_ids']\n    return inputs\n\n# Tokenize datasets\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nvalidation_dataset = validation_dataset.map(tokenize_function, batched=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps=500,  # Evaluate every 500 steps\n    logging_dir='./logs',\n    logging_steps=100,\n    learning_rate=3e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    save_strategy=\"steps\",\n    save_steps=500,  # Save model every 500 steps\n    fp16=True,  # Mixed precision training\n    report_to=\"none\"  # Avoid unwanted logging\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n)\n\n# Start training\ntrainer.train()\n\n# Save the final model\ntrainer.save_model(\"./final_t5_model\")\ntokenizer.save_pretrained(\"./final_t5_model\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T04:47:10.350034Z","iopub.execute_input":"2024-11-19T04:47:10.350317Z","iopub.status.idle":"2024-11-19T05:00:30.875918Z","shell.execute_reply.started":"2024-11-19T04:47:10.35029Z","shell.execute_reply":"2024-11-19T05:00:30.874762Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fc92fa6188d4238959aa999923d78fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/8.43M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd913930504049df8a65db3c9f94eb09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f4231603cc64070865b4dc5e1bbca57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36eac21b86fc4c58b351f510c0cb5e21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/49401 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ef5757800240a2b4efc41376dfa560"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"182b1eb68730424cae33012d4b834765"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ad8e153304949698a6e3f74dabab284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a895fbc5806648feae6a2d74c54aa9c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfe6c21387d545a5a4fcef1f9344f117"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aca401daf36469eb733ed404e2d3929"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47975ca79d704cb5b94922447a7501c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2129a2737104e1a8dd1602289517595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa97e18eaef349b78594eb93dcba7212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3539 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bfd203d2cd441f885a71ae9b131199f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1875/1875 12:41, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.138000</td>\n      <td>0.125653</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.120800</td>\n      <td>0.119192</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.113100</td>\n      <td>0.116715</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"('./final_t5_model/tokenizer_config.json',\n './final_t5_model/special_tokens_map.json',\n './final_t5_model/spiece.model',\n './final_t5_model/added_tokens.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"# !pip install evaluate\n# !pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:21:11.809208Z","iopub.execute_input":"2024-11-19T05:21:11.810047Z","iopub.status.idle":"2024-11-19T05:21:11.814072Z","shell.execute_reply.started":"2024-11-19T05:21:11.809995Z","shell.execute_reply":"2024-11-19T05:21:11.813117Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport evaluate\n\n# Load the saved model and tokenizer\nmodel_path = \"./final_t5_model\"\ntokenizer = T5Tokenizer.from_pretrained(model_path)\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define evaluation function for ROUGE and BLEU\ndef evaluate_model(test_data, model, tokenizer, batch_size=16, max_length=128, num_beams=5):\n    # Initialize metrics\n    rouge = evaluate.load(\"rouge\")\n    bleu = evaluate.load(\"bleu\")\n    \n    # Store references and predictions\n    references = []\n    predictions = []\n\n    # Iterate through the test dataset in batches\n    for start_idx in range(0, len(test_data), batch_size):\n        end_idx = min(start_idx + batch_size, len(test_data))\n        batch = test_data[start_idx:end_idx]\n        \n        # Prepare the batch input\n        input_texts = batch[\"input_text\"].tolist()\n        target_texts = batch[\"target_text\"].tolist()\n        \n        # Tokenize the inputs in batch\n        inputs = tokenizer(input_texts, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=True)\n        inputs = inputs.to(device)\n\n        # Generate predictions in batch\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"],\n                max_length=max_length,\n                num_beams=num_beams,\n                early_stopping=True\n            )\n\n        # Decode and store predictions\n        for i in range(len(outputs)):\n            predicted_text = tokenizer.decode(outputs[i], skip_special_tokens=True)\n            references.append([target_texts[i].split()])  # BLEU expects a list of reference lists\n            predictions.append(predicted_text.split())\n\n            # Add to ROUGE metric\n            rouge.add(prediction=predicted_text, reference=target_texts[i])\n\n    # Calculate BLEU and ROUGE scores\n    bleu_score = bleu.compute(predictions=predictions, references=references)\n    rouge_score = rouge.compute()\n\n    # Print results\n    print(\"ROUGE Score:\", rouge_score)\n    print(\"BLEU Score:\", bleu_score[\"bleu\"])\n\n# Evaluate on the test dataset\nevaluate_model(test_data, model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-11-19T08:02:58.481979Z","iopub.execute_input":"2024-11-19T08:02:58.482722Z","iopub.status.idle":"2024-11-19T08:02:58.487849Z","shell.execute_reply.started":"2024-11-19T08:02:58.482687Z","shell.execute_reply":"2024-11-19T08:02:58.486887Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Inference Code (Paraphrase Generation)","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n# Load the saved model and tokenizer\nmodel_path = \"./final_t5_model\"\ntokenizer = T5Tokenizer.from_pretrained(model_path)\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\n\n# Preprocessing function for inference\ndef preprocess_input(sentence):\n    return \"paraphrase: \" + sentence\n\n# Inference function with diversity-enhancing parameters\ndef generate_paraphrase(input_text, model, tokenizer, max_length=128, num_beams=5, num_return_sequences=5, top_k=50, top_p=0.95, temperature=0.7):\n    # Preprocess input\n    input_text = preprocess_input(input_text)\n    \n    # Tokenize input\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=\"max_length\")\n    \n    # Generate paraphrases with sampling\n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_length=max_length,\n        num_beams=num_beams,\n        num_return_sequences=num_return_sequences,\n        top_k=top_k,              # Use top-k sampling\n        top_p=top_p,              # Use top-p (nucleus) sampling\n        temperature=temperature,  # Adjust diversity\n        early_stopping=True\n    )\n    \n    # Decode generated outputs\n    paraphrased_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n    return paraphrased_texts\n\n# Example sentence\ninput_sentence = \"The quick brown fox jumps over the lazy dog.\"\n\n# Generate paraphrases\nparaphrased_sentences = generate_paraphrase(\n    input_sentence, model, tokenizer, num_return_sequences=3\n)\n\n# Display results\nprint(f\"Original: {input_sentence}\")\nfor i, paraphrase in enumerate(paraphrased_sentences, 1):\n    print(f\"Paraphrase {i}: {paraphrase}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:14:24.509322Z","iopub.execute_input":"2024-11-19T05:14:24.509712Z","iopub.status.idle":"2024-11-19T05:14:26.270625Z","shell.execute_reply.started":"2024-11-19T05:14:24.509681Z","shell.execute_reply":"2024-11-19T05:14:26.269719Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Original: The quick brown fox jumps over the lazy dog.\nParaphrase 1: The quick brown fox jumps over the lazy dog.\nParaphrase 2: The quick brown fox leaps over the lazy dog.\nParaphrase 3: The fast brown fox jumps over the lazy dog.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example sentence\ninput_sentence = \"The little boy built a sandcastle by the beach.\"\n\n# Generate paraphrases\nparaphrased_sentences = generate_paraphrase(\n    input_sentence, model, tokenizer, num_return_sequences=3\n)\n\n# Display results\nprint(f\"Original: {input_sentence}\")\nfor i, paraphrase in enumerate(paraphrased_sentences, 1):\n    print(f\"Paraphrase {i}: {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:14:34.606871Z","iopub.execute_input":"2024-11-19T05:14:34.607946Z","iopub.status.idle":"2024-11-19T05:14:36.012485Z","shell.execute_reply.started":"2024-11-19T05:14:34.607909Z","shell.execute_reply":"2024-11-19T05:14:36.011477Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Original: The little boy built a sandcastle by the beach.\nParaphrase 1: The little boy built a sandcastle by the beach.\nParaphrase 2: The little boy built a sandcastle on the beach.\nParaphrase 3: The little boy built a sandcastle near the beach.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example sentence\ninput_sentence = \"She enjoys reading books on rainy afternoons.\"\n\n# Generate paraphrases\nparaphrased_sentences = generate_paraphrase(\n    input_sentence, model, tokenizer, num_return_sequences=3\n)\n\n# Display results\nprint(f\"Original: {input_sentence}\")\nfor i, paraphrase in enumerate(paraphrased_sentences, 1):\n    print(f\"Paraphrase {i}: {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:14:43.482795Z","iopub.execute_input":"2024-11-19T05:14:43.483109Z","iopub.status.idle":"2024-11-19T05:14:44.544091Z","shell.execute_reply.started":"2024-11-19T05:14:43.483081Z","shell.execute_reply":"2024-11-19T05:14:44.543084Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Original: She enjoys reading books on rainy afternoons.\nParaphrase 1: She enjoys reading books on rainy afternoons.\nParaphrase 2: She enjoys reading on rainy afternoons.\nParaphrase 3: She loves reading books on rainy afternoons.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example sentence\ninput_sentence = \"The dog barked loudly at the stranger outside the house.\"\n\n\n# Generate paraphrases\nparaphrased_sentences = generate_paraphrase(\n    input_sentence, model, tokenizer, num_return_sequences=3\n)\n\n# Display results\nprint(f\"Original: {input_sentence}\")\nfor i, paraphrase in enumerate(paraphrased_sentences, 1):\n    print(f\"Paraphrase {i}: {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:14:53.772153Z","iopub.execute_input":"2024-11-19T05:14:53.77284Z","iopub.status.idle":"2024-11-19T05:14:55.060947Z","shell.execute_reply.started":"2024-11-19T05:14:53.772806Z","shell.execute_reply":"2024-11-19T05:14:55.060109Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Original: The dog barked loudly at the stranger outside the house.\nParaphrase 1: The dog barked loudly at the stranger outside the house.\nParaphrase 2: The dog barked loudly at a stranger outside the house.\nParaphrase 3: The dog loudly barked at the stranger outside the house.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Example sentence\ninput_sentence = \"Climate change is one of the most pressing issues of our time.\"\n\n\n# Generate paraphrases\nparaphrased_sentences = generate_paraphrase(\n    input_sentence, model, tokenizer, num_return_sequences=3\n)\n\n# Display results\nprint(f\"Original: {input_sentence}\")\nfor i, paraphrase in enumerate(paraphrased_sentences, 1):\n    print(f\"Paraphrase {i}: {paraphrase}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-19T05:15:02.704303Z","iopub.execute_input":"2024-11-19T05:15:02.704684Z","iopub.status.idle":"2024-11-19T05:15:03.97477Z","shell.execute_reply.started":"2024-11-19T05:15:02.704649Z","shell.execute_reply":"2024-11-19T05:15:03.973781Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Original: Climate change is one of the most pressing issues of our time.\nParaphrase 1: Climate change is one of the most pressing issues of our time.\nParaphrase 2: Climate change is one of the most pressing issues of our time .\nParaphrase 3: The climate change is one of the most pressing issues of our time.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}